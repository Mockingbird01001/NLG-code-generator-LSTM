{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os, uuid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# import horovod.spark.keras as hvd\n",
    "# from horovod.spark.common.store import Store\n",
    "\n",
    "import functions as f\n",
    "from Text import *\n",
    "from LSTM_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= #\n",
    "# CODE GENERATION WITH LSTM #\n",
    "# ========================= #\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore') # ignorer les signes lol !\n",
    "warnings.simplefilter(action='ignore',  category=FutureWarning) # cacher les alerts (les ignorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducability\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# init spark session\n",
    "# ajout du GPU dans notre spark\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName('NLG_with_LSTM').setMaster('local[*]')\n",
    "conf.set(\"spark.executor.resource.gpu.amount\", '4')\n",
    "conf.set(\"spark.task.resource.gpu.amount\", '1')\n",
    "conf.set(\"spark.sql.shuffle.partitions\", '16')\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "spark = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d82548aa0c24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NLG_with_LSTM</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=NLG_with_LSTM>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing \n",
    "cree un merge et un vocab essentiel a la prediction final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du jeu de fichiers\n",
    "input_train = f.read_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 2 # sequence len by\n",
    "step = 1 # le pas {avance de 1 mot a chaque sequence}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is split into sequences of length 2 (max_len parameter) with step 1. We can see that the first sequence of 2 words starts with the first (0-index) word and the second sequence starts after 1 words, so from the 2nd word (1-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des caracteres 5568656\n",
      "total tokens: 442748, distinct tokens: 88118\n",
      "number of sequences of length 2: 442746\n"
     ]
    }
   ],
   "source": [
    "print(\"Total des caracteres\", len(input_train))\n",
    "text_train = Text(input_train)\n",
    "text_train.tokens_info()\n",
    "\n",
    "seq_train = Sequences(text_train, max_len, step)\n",
    "seq_train.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'numpy', 'as', 'np', 'from', 'tensorflow.python.framework', 'import', 'dtypes', 'from', 'tensorflow.python.framework']\n",
      "[3545, 65639, 58014, 81302, 26162, 47942, 3545, 9652, 26162, 47942]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3545, 65639],\n",
       "       [65639, 58014]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text_train.tokens[:10])\n",
    "print(text_train.tokens_ind[:10])\n",
    "\n",
    "np.array(seq_train.sequences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataGenerator is a Python generator that outputs batches of data (sequences and corresponding next words). \n",
    "Since the vocabulary size is over 800K, it's impossible to fit all data to the memory and that's why **batch generator** is extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # nombre de sequence a prendre en compte dans le fit\n",
    "layer_size = 64 # nombre de neuronnes\n",
    "nb_epoch = 10\n",
    "\n",
    "params = {\n",
    "    'sequence_length': max_len,\n",
    "    'vocab_size': len(text_train),\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "# train_generator = TextDataGenerator(spark=spark, sequences=seq_train.sequences, next_words=seq_train.next_words, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model\n",
    "\n",
    "We'll build a simple model with one LSTM layer, dropout and dense layer with softmax activation (to return word probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pour la creation de notre model\n",
    "def lstm_model(sequence_length, vocab_size, layer_size, embedding=False):\n",
    "    model = models.Sequential()\n",
    "    if embedding:\n",
    "        model.add(layers.Embedding(vocab_size, layer_size))\n",
    "        model.add(layers.Bidirectional(layers.LSTM(layer_size)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    else:\n",
    "        model.add(layers.LSTM(layer_size, input_shape=(sequence_length, vocab_size)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(vocab_size, activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation du model\n",
    "model = lstm_model(sequence_length=max_len, vocab_size=len(text_train), layer_size=layer_size)\n",
    "\n",
    "# initialisation de l'optimizer\n",
    "# optimizer = optimizers.Adamax(learning_rate=0.01)\n",
    "# optimizer = optimizers.RMSprop(learning_rate=0.01)\n",
    "# optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# initialisation de le la loss function\n",
    "# loss = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# compile our model\n",
    "# model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                22574848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 88118)             5727670   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,302,518\n",
      "Trainable params: 28,302,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# afficher un recap des parametres de chaque couche\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# désactiver le GPU lors de la construction du modèle pour éviter le debordement de la mémoire\n",
    "if LooseVersion(tf.__version__) >= LooseVersion('2.0.0'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    keras.backend.set_session(tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit le model (train) spark - elephas\n",
    "# spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "\n",
    "# lancement du fit avec toutes les données\n",
    "# spark_model.fit(train_generator.generate_rdds(), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)\n",
    "\n",
    "# lancement du fit avec un seil batch\n",
    "# spark_model.fit(train_generator.generate_1_rdd(index=0), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancement du train avec les couches LSTM et dropout avec keras\n",
    "model.fit(train_generator, batch_size=batch_size, steps_per_epoch=len(train_generator), epochs=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du model\n",
    "# model.save('data/out/lstm_model_simple')\n",
    "#f.save_pickle(model, 'data/pkl/lstm_model_simple')\n",
    "\n",
    "# load un ancien model\n",
    "model = models.load_model('data/out/lstm_model_simple')\n",
    "# model = f.load_pickle('data/out/lstm_model_simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with LSTM model\n",
    "\n",
    "Generating text with LSTM model requires building the prediction loop which starts with choosing a prefix and setting the number of words to generate. Then we need to predict the next word using our LSTM model and use this word as part of the prefix for the next model input. The loop is executed until the expected number of words is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2ind, ind2token = text_train.token2ind, text_train.ind2token\n",
    "# sequence initiale\n",
    "input_prefix = \"\"\"\n",
    "    from tensorflow.python.framework import dtypes\n",
    "\"\"\"\n",
    "# tokenization de la sequence initiale\n",
    "text_prefix = Text(input_prefix, token2ind, ind2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction a partir d'une sequence\n",
    "pred = ModelPredict(model, text_prefix, token2ind, ind2token, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 1\n",
      "from tensorflow.python.framework import dtypes count_val) name='output')(x) trivial shard_index create_variable_like_keras_layer( random_ops.random_normal([1, dtypes.as_dtype(out_type) 46]]]], `suffix`. summary_ops.set_step(42) 8]).astype(np.float32) self.all_reduce( self.assertEqual(xla_shape.dimensions(), array_ops.split(c, assertAllEqualUnicode(self, degradation.\" row_lengths=[4, arguments, FLAG_NAME_EXCLUDED_OPNAMES) testRunUIWithOnUIExitCallback(self): checkpoint.\") `{self.__class__.__name__}.summary()` is_both_nonscalar num_bits=8, allowed return_state=False, enumerate(inputs): super(XlaDeviceGpuTest, convertible(tensor, new_restore_ops(self, CustomUnhashable: self._sharding_policies) self.collect_summary_per_core 6.02427039364742014255E0, row_partition=self._row_partition.with_dtype(dtype),\"delimiter DT_STRING compatibility, trt_convert Context. tf.compat.v1.placeholder( _inspect.isbuiltin(tf_decorator.unwrap(object)[1]) get_lib()) bounding tape: `run_eagerly` snapshot.op.type `Coordinator`,'tf.random_uniform':\n",
      "\n",
      "\n",
      "temperature: 0.7\n",
      "from tensorflow.python.framework import dtypes full_type_pb2.TFT_RAGGED ragged_factory_ops.constant([[[1], keepdims, self.emit('\\nscf.yield layer.__class__.from_config(layer_config) logging.info(line) registration. `GraphKeys.TRAINABLE_VARIABLES` 7)) graphs) tensor_shape.Dimension(20), tensor_trace_order.cache_idx_to_tensor_idx[cache_idx](device_type, rebuilt_func.graph.structured_outputs) new_cast_op_name_to_node_map self.cell validate_training=True, resource_tracker: resource_variable_ops.resource_scatter_sub( values)\") data_flow_ops.RandomShuffleQueue( row_partition=row_partition) f\"({initial_value.shape}) restore_events gast.Store): dtype=dtypes.string), self._share_variables: keyed self.assertIn(signature_key, state `var_name_to_vocab_info` returned.append(self.evaluate(get_next()))'greedy', encoded_flats) value=[5., output_arrays spec_1.shape, _REASON_UNSAFE_OP)) array_ops.depth_to_space( imin self._ZlibDecompressFile(fn, hidden_weights:\"\"\"Random-number global_batch_size`\")\"v2\" _XLA_OP_BY_OP_INPUTS_LIMIT: _GetSvdGradGradOpTest(dtype_, r\"\\\\\\1\") help=(\"Upgrade self._get_first_op_from_collection(ops.GraphKeys.READY_OP)\n",
      "\n",
      "\n",
      "temperature: 0.4\n",
      "from tensorflow.python.framework import dtypes dataset_ops.Dataset.from_tensor_slices(self._filenames) int)): 1.1) O(exp(-0.5 no_rank_spec])( row_length, unused_attribute_messages.append( resolver.TPUClusterResolver convert. @test_util.assert_no_new_tensors versions[0].to_proto() testcase_name=\"list_with_submessages\", checkpoint_utils rpc_layer=None,\"v\" `serialize`. super(EagerClusterReplicateTest,\"use min(parent_rank, self._reduce_window( name=slot_variable_names.ms, symbol._tf_api_names 8]]))), sx1 re.match(r\"^cuda _clone_export_output_with_tensors input_h=init_h, sub_w_last_node w_read type(b) resolved TypeError( _GetSvdOpTest(dtype, tensor_util.is_tf_type(output): TensorArray): prefer new_impl._size attr_value_type=\"s\") ops.CrossReplicaSum(ops.Constant(c, func(x, aggregated versions.GRAPH_DEF_VERSION_MIN_CONSUMER extension zip(*[(y, to\\n\"\"`reduce_op`={reduce_op} z testNpXent(self): var_list self.assertEqual(p.get_sharded_shape([4,\n",
      "\n",
      "\n",
      "temperature: 0.1\n",
      "from tensorflow.python.framework import dtypes op.get_attr(\"dtype\") debugger_cli_common.RichLine(\"|\") [[8]]], \\[3\\]\") graphs[1].get_tensor_by_name(\"sync_rep_local_step:0\") self._compare( `graph` **{prop: loc=[[5.], testStringSplitWithDelimiterTensor(self): row_splits.numpy()) global_tt_summary_cache):'user':(%d/%d).', target_name='my_call', API) constant_op.constant(1, 21, functools.partial(wrap, placement_function(i). keys) dequeued_t[1].get_shape().as_list()) >0') gast.Expr): self.skipTest(\"Legacy(gast.Name, tpu.replicate np.int32(2), @tf_export(v1=[\"data.experimental.CsvDataset\"]) though. tf.ragged.constant([[9, tf_config[key] core.Tensor]: _xla.get_tpu_client( instead\", dummy_*'.format( summary_pb2.SummaryMetadata()'testing', print(element) extra_handle_data'ProximalAdagradSlotVariableNames', scatter permuted) collections: TypeError, learning_rates=learning_rates, scatter_div(self, dimensions=[1, symbol.__dict__: Incremented\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures = [1, 0.7, 0.4, 0.1] # initialisation de la liste de temperature\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(pred.generate_sequence(50, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with LSTM model with Embedding layer\n",
    "\n",
    "The previous model was taking as an input the sequences of words represented as one-hot vectors. In the second approach, we'll feed indexes of words to the model and train the Embedding layers which will create word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_emb = params.copy() # on recopie les parametres definie pus haut\n",
    "params_emb['embedding'] = True # on initialise embedding a true pour utiliser to_categorical()\n",
    "\n",
    "train_generator_emb = TextDataGenerator(spark, seq_train.sequences, seq_train.next_words, **params_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un nouveau model avec les couche embedding\n",
    "model_emb = lstm_model(max_len, len(text_train), layer_size, embedding=True)\n",
    "\n",
    "# initialisation de l'optimizer\n",
    "# optimizer = optimizers.Adamax(learning_rate=0.01)\n",
    "# optimizer = optimizers.RMSprop(learning_rate=0.01)\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# initialisation de le la loss function\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# model_emb.compile(loss='BinaryCrossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_emb.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          5639552   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 88118)             11367222  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,072,822\n",
      "Trainable params: 17,072,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# afficher un recap des parametres de chaque couche\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit le model (train) spark - elephas\n",
    "spark_model = SparkModel(model_emb, frequency='epoch', mode='asynchronous')\n",
    "\n",
    "# lancement du fit avec toutes les données\n",
    "# spark_model.fit(train_generator.generate_rdds(), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)\n",
    "\n",
    "# lancement du fit avec un seil batch\n",
    "# spark_model.fit(train_generator.__getitem__(0), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 327s 3s/step - loss: 1.1304e-05 - accuracy: 0.0528\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 340s 3s/step - loss: 1.1290e-05 - accuracy: 0.0537\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 347s 3s/step - loss: 1.1288e-05 - accuracy: 0.0537\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 349s 3s/step - loss: 1.1285e-05 - accuracy: 0.0537\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 360s 3s/step - loss: 1.1280e-05 - accuracy: 0.0537\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 356s 3s/step - loss: 1.1271e-05 - accuracy: 0.0587\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 355s 3s/step - loss: 1.1254e-05 - accuracy: 0.0680\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 350s 3s/step - loss: 1.1229e-05 - accuracy: 0.0711\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 371s 3s/step - loss: 1.1198e-05 - accuracy: 0.0739\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 366s 3s/step - loss: 1.1171e-05 - accuracy: 0.0788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f342fd550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lancement du train avec les couches embedding et LSRM\n",
    "model_emb.fit(train_generator_emb, batch_size=batch_size, steps_per_epoch=len(train_generator_emb), epochs=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/out/lstm_model_emb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: data/out/lstm_model_emb/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f344f2e20> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f34432850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e222d27-efcf-41e6-a95f-245684851bcb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2e222d27-efcf-41e6-a95f-245684851bcb/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f344f2e20> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f1f34432850> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# sauvegarde du model\n",
    "model_emb.save('data/out/lstm_model_emb')\n",
    "f.save_pickle(model_emb, 'data/pkl/lstm_model_emb')\n",
    "\n",
    "# load un ancien model\n",
    "# model_emb = models.load_model('data/out/lstm_model_emb')\n",
    "# model_emb = f.load_pickle('data/out/lstm_model_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2ind, ind2token = text_train.token2ind, text_train.ind2token\n",
    "# sequence initiale\n",
    "input_prefix = \"\"\"from tensorflow.python.framework import ops\"\"\"\n",
    "# tokenization de la sequence initiale\n",
    "text_prefix = Text(input_prefix, token2ind, ind2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation des predictions a partir de la meme sequence\n",
    "pred_emb = ModelPredict(model_emb, text_prefix, token2ind, ind2token, max_len, embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature: 1\n",
      "from tensorflow.python.framework import ops np_input\"GetSessionHandle self.run_and_report_benchmark( centered: {y:.4f}\".format(**batch)) self.assertAllEqual(grads_alpha_val.shape, self.assertEqual([1.], \\[\\]\"): parameters.adagrad_momentum.epsilon space math_ops.cast(beta, `dataset` f(*args) 0.30485708, all(t.dtype(`tf.summary.trace_on`, @tf.function(input_signature=[[(handle_name, s_1 {\"sizes\": @dispatch.dispatch_for_api(nn_ops.dropout_v2) no_op b\"abcd\"]]), linear_initializer match.group('prefix_name') op.device) ops). Coming math_ops.greater_equal(axis, RaggedTensorSpec(self._shape[1:], v2.read_value()'tf.nn.convolution': reversed(_TYPE_CONVERSION_FUNCTION_REGISTRY): custom_objects=None, shape=[None]) predicate. tensor[\"data_buffer\"] self.assertAllEqual(np.array([[b\"a\", dim_size_dtype) key[0].step\"AVERAGE_POOL_2D\" print(final_carry_state.shape) key_to_promote self._get_first_op_from_collection(ops.GraphKeys.READY_OP) [constant_op.constant(1.),'PreventGradient', variables.Variable([True, 0.999, 4)] _ChainAllNodes(graph_def):\n",
      "\n",
      "\n",
      "temperature: 0.7\n",
      "from tensorflow.python.framework import ops show_memory=show_memory, xla_ops.xla_cluster_output(o, tensor_spec.TensorSpec([5]), benchmarkTridiagonalMulOp(self): self.cell.built: abnormal DeviceAssignment base_case)): v.scatter_add(value) dataset_ops.DatasetV2.from_tensors([0, new_value.body[0].value table.optimizer'Previously count: float: right.shape)) T=ref.dtype, num_parallel_calls=num_parallel_parser_calls) StructuredTensor.from_fields(shape=[2, TensorSpec( self.assertAllEqual(eager_output.numpy(), enumerate(sample_weights):'grpc://10.120.27.7:8470,' node_stat_dims]) len(resource_tracker1.resources)) size=int(medium_size_percentage tuple(literal.shape.dimensions)-1 self.evaluate(gen_string_ops.string_n_grams( attribute_name np.amin(inp, count=count) tile_assignment_dimensions=[1], 12]).to_list() input. NaN'--input_graph', c'], _GetModelPaths(model_class) array) 8.]) fh(core/util/test_log.proto). backprop_util.IsTrainable(element_type) `'grpc://hostname:port'` fetch(self): condition. `tensor.numpy()`. prev_v1_momentum_val) self.evaluate(uniform.entropy()))\n",
      "\n",
      "\n",
      "temperature: 0.4\n",
      "from tensorflow.python.framework import ops string_value_list self.flat_values_spec tf.Const new_cache_shape.extend(cache_variable.shape.as_list()) half_pixel_centers=parameters[\"half_pixel_centers\"]) self._tf_reduce(x, allow_build_at_runtime=allow_build_at_runtime) expected=[[[1 summarize=None, FullArgSpec array_ops.constant( _overloaded_operator(\"__mul__\") self.assertRaises(TypeError, feed name.startswith(method_prefix) file2 Uniform(distribution.Distribution): xla_client.shape_from_pyval(np.array(0, out_capturer.getvalue() parameters.gradient_accumulation_status v1=['invalid']) assign_sub(var, broadcast_dim_sizes: [None, self._grad_source_for_name(\"foo/gradients\")) np.array(30.0)], _check_embedding_and_slot_variables_for_adagrad(self,'tf.div': array_ops.stack([t1, summary.merge_all(\"foo_key\") ValueError('Function collection_def.node_list.value 5e-4, compatible_values_fn, execution, [tf.keras.layers.Embedding](\"Nest\", diags_matrix_batch, detected.'tf.debugging.is_finite': add_control_dependencies=add_control_dependencies, aggregation=variables_lib.VariableAggregation.SUM) _cache_dispatch(self, defined_in) q.enqueue((10.,)).run()\"V1\" RowPartition.'tf.keras.layers.InputSpec', parts[0] dict(indices=[[1]],\n",
      "\n",
      "\n",
      "temperature: 0.1\n",
      "from tensorflow.python.framework import ops location_tag logdir'tf.math.divide_no_nan', dst_size) small_shapes testStructuredTensorArrayLike(self): tf.keras.optimizers.Adagrad(learning_rate=0.1) testZerosLikeObjectAlt(self, ops.NotDifferentiable(\"RegexReplace\") data=meta) array_ops.identity(file_prefix) calculating-np.inf], library. s.format(self.shape, CompositeTensor.\") self._label_count ops.get_default_graph() [1.]) testRunMetadata_wholeRunMetadata(self): set(field_specs)): name=\"range\"): ragged_tensor_to_string(rt3, len(num_cores_per_host_set) new_restore_ops ['{}:{}'.format(val, XentOpTestBase(test.TestCase): goes prefixed'tf.sparse.retain', 810 @tf_export(\"strings.unicode_decode\") _current_tpu_context.set_number_of_shards(None)'tf.train.batch': testUnpack(self): self.evaluate(nn_ops.softplus(sigma)), Sequence):)) ndims_name='rank(input_tensor)')\"StaticEngine\" </div> result.extend(field_spec._to_batched_tensor_list(field_value)) errors_equivalent): ALT_JOB_NAME'coo_sparse', RaggedTensorDynamicShape) values=values) filesystem. num_features=None, name='y',\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures = [1, 0.7, 0.4, 0.1] # initialisation de la liste de temperature\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(pred_emb.generate_sequence(50, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
