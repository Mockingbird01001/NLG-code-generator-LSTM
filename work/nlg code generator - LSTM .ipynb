{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, os, uuid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "# import horovod.spark.keras as hvd\n",
    "# from horovod.spark.common.store import Store\n",
    "\n",
    "import functions as f\n",
    "from Text import *\n",
    "from LSTM_class import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= #\n",
    "# CODE GENERATION WITH LSTM #\n",
    "# ========================= #\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore') # ignorer les signes lol !\n",
    "warnings.simplefilter(action='ignore',  category=FutureWarning) # cacher les alerts (les ignorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducability\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# init spark session\n",
    "# ajout du GPU dans notre spark\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setAppName('NLG_with_LSTM').setMaster('local[*]')\n",
    "conf.set(\"spark.executor.resource.gpu.amount\", '4')\n",
    "conf.set(\"spark.task.resource.gpu.amount\", '1')\n",
    "conf.set(\"spark.sql.shuffle.partitions\", '16')\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "spark = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing \n",
    "cree un merge et un vocab essentiel a la prediction final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture du jeu de fichiers\n",
    "input_train = f.read_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 2 # sequence len by\n",
    "step = 1 # le pas {avance de 1 mot a chaque sequence}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is split into sequences of length 2 (max_len parameter) with step 1. We can see that the first sequence of 2 words starts with the first (0-index) word and the second sequence starts after 1 words, so from the 2nd word (1-index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total des caracteres\", len(input_train))\n",
    "text_train = Text(input_train)\n",
    "text_train.tokens_info()\n",
    "\n",
    "seq_train = Sequences(text_train, max_len, step)\n",
    "seq_train.sequences_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(text_train.tokens[:10])\n",
    "print(text_train.tokens_ind[:10])\n",
    "\n",
    "np.array(seq_train.sequences[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextDataGenerator is a Python generator that outputs batches of data (sequences and corresponding next words). \n",
    "Since the vocabulary size is over 800K, it's impossible to fit all data to the memory and that's why **batch generator** is extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096 # nombre de sequence a prendre en compte dans le fit\n",
    "layer_size = 64 # nombre de neuronnes\n",
    "nb_epoch = 2\n",
    "\n",
    "params = {\n",
    "    'sequence_length': max_len,\n",
    "    'vocab_size': len(text_train),\n",
    "    'batch_size': batch_size,\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "train_generator = TextDataGenerator(spark=spark, sequences=seq_train.sequences, next_words=seq_train.next_words, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LSTM model\n",
    "\n",
    "We'll build a simple model with one LSTM layer, dropout and dense layer with softmax activation (to return word probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pour la creation de notre model\n",
    "def lstm_model(sequence_length, vocab_size, layer_size, embedding=False):\n",
    "    model = models.Sequential()\n",
    "    if embedding:\n",
    "        model.add(layers.Embedding(vocab_size, layer_size))\n",
    "        model.add(layers.Bidirectional(layers.LSTM(layer_size)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    else:\n",
    "        model.add(layers.LSTM(layer_size, input_shape=(sequence_length, vocab_size)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(vocab_size, activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation du model\n",
    "model = lstm_model(sequence_length=max_len, vocab_size=len(text_train), layer_size=layer_size)\n",
    "\n",
    "# initialisation de l'optimizer\n",
    "# optimizer = optimizers.Adamax(learning_rate=0.01)\n",
    "# optimizer = optimizers.RMSprop(learning_rate=0.01)\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# initialisation de le la loss function\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# compile our model\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher un recap des parametres de chaque couche\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# désactiver le GPU lors de la construction du modèle pour éviter le debordement de la mémoire\n",
    "if LooseVersion(tf.__version__) >= LooseVersion('2.0.0'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    keras.backend.set_session(tf.Session(config=tf.ConfigProto(device_count={'GPU': 0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit le model (train) spark - elephas\n",
    "# spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "\n",
    "# lancement du fit avec toutes les données\n",
    "# spark_model.fit(train_generator.generate_rdds(), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)\n",
    "\n",
    "# lancement du fit avec un seil batch\n",
    "# spark_model.fit(train_generator.generate_1_rdd(index=0), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancement du train avec les couches LSTM et dropout avec keras\n",
    "model.fit(train_generator, batch_size=batch_size, steps_per_epoch=len(train_generator), epochs=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du model\n",
    "# model.save('data/out/lstm_model_simple')\n",
    "#f.save_pickle(model, 'data/pkl/lstm_model_simple')\n",
    "\n",
    "# load un ancien model\n",
    "model = models.load_model('data/out/lstm_model_simple')\n",
    "# model = f.load_pickle('data/out/lstm_model_simple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with LSTM model\n",
    "\n",
    "Generating text with LSTM model requires building the prediction loop which starts with choosing a prefix and setting the number of words to generate. Then we need to predict the next word using our LSTM model and use this word as part of the prefix for the next model input. The loop is executed until the expected number of words is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2ind, ind2token = text_train.token2ind, text_train.ind2token\n",
    "# sequence initiale\n",
    "input_prefix = \"\"\"\n",
    "    from tensorflow.python.framework import dtypes\n",
    "    from tensorflow.python.framework import ops\n",
    "\"\"\"\n",
    "# tokenization de la sequence initiale\n",
    "text_prefix = Text(input_prefix, token2ind, ind2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction a partir d'une sequence\n",
    "pred = ModelPredict(model, text_prefix, token2ind, ind2token, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temperatures = [1, 0.7, 0.4, 0.1] # initialisation de la liste de temperature\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(pred.generate_sequence(50, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation with LSTM model with Embedding layer\n",
    "\n",
    "The previous model was taking as an input the sequences of words represented as one-hot vectors. In the second approach, we'll feed indexes of words to the model and train the Embedding layers which will create word representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_emb = params.copy() # on recopie les parametres definie pus haut\n",
    "params_emb['embedding'] = True # on initialise embedding a true pour utiliser to_categorical()\n",
    "\n",
    "train_generator_emb = TextDataGenerator(spark, seq_train.sequences, seq_train.next_words, **params_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un nouveau model avec les couche embedding\n",
    "model_emb = lstm_model(max_len, len(text_train), layer_size, embedding=True)\n",
    "\n",
    "# initialisation de l'optimizer\n",
    "# optimizer = optimizers.Adamax(learning_rate=0.01)\n",
    "# optimizer = optimizers.RMSprop(learning_rate=0.01)\n",
    "optimizer = optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# initialisation de le la loss function\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "\n",
    "# model_emb.compile(loss='BinaryCrossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model_emb.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher un recap des parametres de chaque couche\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit le model (train) spark - elephas\n",
    "spark_model = SparkModel(model_emb, frequency='epoch', mode='asynchronous')\n",
    "\n",
    "# lancement du fit avec toutes les données\n",
    "# spark_model.fit(train_generator.generate_rdds(), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)\n",
    "\n",
    "# lancement du fit avec un seil batch\n",
    "# spark_model.fit(train_generator.__getitem__(0), epochs=nb_epoch, batch_size=batch_size, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancement du train avec les couches embedding et LSRM\n",
    "# model_emb.fit(train_generator_emb, batch_size=batch_size, steps_per_epoch=len(train_generator_emb), epochs=nb_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde du model\n",
    "# model_emb.save('data/out/lstm_model_emb')\n",
    "# f.save_pickle(model_emb, 'data/out/lstm_model_emb')\n",
    "\n",
    "# load un ancien model\n",
    "# model_emb = models.load_model('data/out/lstm_model_emb')\n",
    "# model_emb = f.load_pickle('data/out/lstm_model_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2ind, ind2token = text_train.token2ind, text_train.ind2token\n",
    "# sequence initiale\n",
    "input_prefix = \"\"\"\n",
    "    from tensorflow.python.framework import dtypes\n",
    "    from tensorflow.python.framework import ops\n",
    "\"\"\"\n",
    "# tokenization de la sequence initiale\n",
    "text_prefix = Text(input_prefix, token2ind, ind2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation des predictions a partir de la meme sequence\n",
    "pred_emb = ModelPredict(model_emb, text_prefix, token2ind, ind2token, max_len, embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [1, 0.7, 0.4, 0.1] # initialisation de la liste de temperature\n",
    "\n",
    "for temperature in temperatures:\n",
    "    print('temperature:', temperature)\n",
    "    print(pred.generate_sequence(50, temperature=temperature))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
